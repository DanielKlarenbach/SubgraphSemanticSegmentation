{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUsage:\\n1. Directory with data sets must be placed in directory named \"input\"\\n2. Input directory must be in the same directory as the .ipynb file with this sript\\n3. Inside each data set directory there must be three directories named: images, labels and masks.\\n4. Filenames of labels and masks files must be the same as image filename.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Usage:\n",
    "1. Directory with data sets must be placed in directory named \"input\"\n",
    "2. Input directory must be in the same directory as the .ipynb file with this sript\n",
    "3. Inside each data set directory there must be three directories named: images, labels and masks.\n",
    "4. Filenames of labels and masks files must be the same as image filename.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kgcnn.literature.GraphSAGE import make_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "keras-unet init: TF version is >= 2.0.0 - using `tf.keras` instead of `Keras`\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, UpSampling2D, Input, concatenate\n",
    "from tensorflow.keras import backend as K\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.compat.v1 import ConfigProto\n",
    "#from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras_unet.metrics import iou, iou_thresholded\n",
    "from keras_unet.losses import jaccard_distance\n",
    "\n",
    "\n",
    "data_gen_args = dict(\n",
    "        rotation_range=10.,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        shear_range=10,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False,\n",
    "        fill_mode='nearest'\n",
    ")\n",
    "\n",
    "def showOpencvImage(image, isGray=False):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.show()\n",
    "\n",
    "def get_augmented(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    X_val=None,\n",
    "    Y_val=None,\n",
    "    batch_size=32, \n",
    "    seed=0, \n",
    "    data_gen_args = dict(\n",
    "        rotation_range=10.,\n",
    "        width_shift_range=0.02,\n",
    "        height_shift_range=0.02,\n",
    "        zca_whitening = False,\n",
    "        zca_epsilon = 1e-6,\n",
    "        shear_range=5,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False,\n",
    "        fill_mode='nearest'\n",
    "    )):\n",
    "\n",
    "\n",
    "    # Train data, provide the same seed and keyword arguments to the fit and flow methods\n",
    "    X_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    Y_datagen_1 = ImageDataGenerator(**data_gen_args)\n",
    "    Y_datagen_2 = ImageDataGenerator(**data_gen_args)\n",
    "    X_datagen.fit(X_train, augment=True, seed=seed)\n",
    "    Y_train_1 = Y_train[:,:,:,0:1]\n",
    "    Y_train_2 = Y_train[:,:,:,1:2]\n",
    "    Y_datagen_1.fit(Y_train_1, augment=True, seed=seed)\n",
    "    Y_datagen_2.fit(Y_train_2, augment=True, seed=seed)\n",
    "    X_train_augmented = X_datagen.flow(X_train, batch_size=batch_size, shuffle=True, seed=seed)\n",
    "    Y_train_augmented_1 = Y_datagen_1.flow(Y_train_1, batch_size=batch_size, shuffle=True, seed=seed)\n",
    "    Y_train_augmented_2 = Y_datagen_2.flow(Y_train_2, batch_size=batch_size, shuffle=True, seed=seed)\n",
    "    \n",
    "    train_generator = zip(X_train_augmented, Y_train_augmented_1, Y_train_augmented_2)#, Y_train_augmented_3)\n",
    "    return train_generator\n",
    "\n",
    "def my_generator(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    train_gen,\n",
    "    X_val=None,\n",
    "    Y_val=None,\n",
    "    batch_size=2, \n",
    "    seed=0, \n",
    "    data_gen_args = dict(\n",
    "        rotation_range=10.,\n",
    "        width_shift_range=0.02,\n",
    "        height_shift_range=0.02,\n",
    "        zca_whitening = False,\n",
    "        zca_epsilon = 1e-6,\n",
    "        shear_range=5,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False,\n",
    "        fill_mode='nearest'\n",
    "    )):\n",
    "    while 1:\n",
    "        sample_batch = next(train_gen)\n",
    "        xx, yy1,yy2 = sample_batch\n",
    "        yy = np.zeros((xx.shape[0],xx.shape[1],xx.shape[2],2),dtype=np.float32)\n",
    "        yy[:,:,:,0:1] = yy1\n",
    "        yy[:,:,:,1:2] = yy2\n",
    "#        yy[:,:,:,6:7] = yy3\n",
    "        yield (xx, yy)\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def upsample_conv(filters, kernel_size, strides, padding):\n",
    "    return Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n",
    "\n",
    "def upsample_simple(filters, kernel_size, strides, padding):\n",
    "    return UpSampling2D(strides)\n",
    "\n",
    "def conv2d_block(\n",
    "    inputs, \n",
    "    filters=16, \n",
    "    kernel_size=(3,3), \n",
    "    activation='tanh', \n",
    "#    kernel_initializer='he_normal', \n",
    "    kernel_initializer= 'glorot_uniform',\n",
    "    padding='same'):\n",
    "    \n",
    "    c = Conv2D(filters, kernel_size, activation=activation, kernel_initializer=kernel_initializer, padding=padding) (inputs)\n",
    "    c = Conv2D(filters, kernel_size, activation=activation, kernel_initializer=kernel_initializer, padding=padding) (c)\n",
    "    return c\n",
    "\n",
    "def my_custom_unet(\n",
    "    input_shape,\n",
    "    num_classes=1,\n",
    "    upsample_mode='deconv', # 'deconv' or 'simple' \n",
    "    filters=16,\n",
    "    num_layers=4,\n",
    "    output_activation='softmax'): # 'sigmoid' or 'softmax'\n",
    "    \n",
    "    if upsample_mode=='deconv':\n",
    "        upsample=upsample_conv\n",
    "    else:\n",
    "        upsample=upsample_simple\n",
    "\n",
    "    # Build U-Net model\n",
    "    inputs = Input(input_shape)\n",
    "    x = inputs   \n",
    "\n",
    "    down_layers = []\n",
    "    for l in range(num_layers):\n",
    "        x = conv2d_block(inputs=x, filters=filters)\n",
    "        down_layers.append(x)\n",
    "        x = MaxPooling2D((2, 2)) (x)\n",
    "        filters = filters*2 # double the number of filters with each layer\n",
    "\n",
    "    x = conv2d_block(inputs=x, filters=196)\n",
    "\n",
    "\n",
    "    for conv in reversed(down_layers):        \n",
    "        filters //= 2 # decreasing number of filters with each layer \n",
    "        x = upsample(filters, (2, 2), strides=(2, 2), padding='same') (x)\n",
    "        x = concatenate([x, conv])\n",
    "        x = conv2d_block(inputs=x, filters=filters)\n",
    "    \n",
    "    outputs = Conv2D(num_classes, (1, 1), activation=output_activation) (x)    \n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "def iou(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "\n",
    "def train_UNET(graphs_train, graphs_test):\n",
    "    images_train = list()\n",
    "    for graph in graphs_train:\n",
    "        images_train.append(f\"{IMAGES_DIR_PATH}/{graph.file_id}.bmp\")\n",
    "\n",
    "    masks = glob.glob(f\"{MASKS_DIR_PATH}/*.bmp\")\n",
    "    orgs = glob.glob(f\"{IMAGES_DIR_PATH}/*.bmp\")\n",
    "\n",
    "    masks.sort()\n",
    "    orgs.sort()\n",
    "\n",
    "    list_train = []\n",
    "    list_test =  []\n",
    "\n",
    "    for (i, org) in enumerate(orgs):\n",
    "        for (j, img_train) in enumerate(images_train):\n",
    "            if org == img_train:\n",
    "                list_train.append(i)\n",
    "                break\n",
    "            elif j == len(images_train)-1:\n",
    "                list_test.append(i)\n",
    "                \n",
    "    print(len(list_train))\n",
    "    print(len(list_test))\n",
    "    print(len(masks))\n",
    "    print(len(orgs))\n",
    "\n",
    "    imgs_list = []\n",
    "    masks_list = []\n",
    "\n",
    "    size = (128,128)\n",
    "\n",
    "    for image, mask in zip(orgs, masks):\n",
    "        im = cv2.imread(image)\n",
    "        im = im[:,:,0]\n",
    "        imgs_list.append(im)\n",
    "\n",
    "        im = cv2.imread(mask)\n",
    "        im = im[:,:,0]\n",
    "\n",
    "        imMask = np.zeros((im.shape[0],im.shape[1],2),dtype=np.float32)\n",
    "        imMask[im == 0,0] = 1               #background\n",
    "        imMask[im!=0,1] = 1                 #spine\n",
    "\n",
    "        masks_list.append(imMask)\n",
    "\n",
    " \n",
    "        \n",
    "    imgs_np = np.asarray(imgs_list)\n",
    "    masks_np = np.asarray(masks_list)\n",
    "    print(imgs_np.shape, masks_np.shape)\n",
    "\n",
    "    weights = np.ones((2),dtype=np.float32)\n",
    "    for i in range(0,2):\n",
    "        weights[i] = 1/(np.sum(masks_np[:,:,:,i])/(masks_np.shape[0]*masks_np.shape[1]*masks_np.shape[2]))\n",
    "\n",
    "    w = sum(weights)\n",
    "    weights = weights/w\n",
    "\n",
    "    print(weights)\n",
    "\n",
    "    print(imgs_np.max(), masks_np.max())\n",
    "    x = np.asarray(imgs_np, dtype=np.float32)\n",
    "    y = np.asarray(masks_np, dtype=np.float32)\n",
    "    print(x.max(), y.max())\n",
    "    print(x.shape, y.shape)\n",
    "    y = y.reshape(y.shape[0], y.shape[1], y.shape[2], 2)\n",
    "    print(x.shape, y.shape)\n",
    "    x = x.reshape(x.shape[0], x.shape[1], x.shape[2], 1)\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "    list_val = list_train[:int(0.2*len(list_train))]\n",
    "    list_train = list_train[int(0.2*len(list_train)):]\n",
    "\n",
    "    print(list_train)\n",
    "    print(list_val)\n",
    "    print(list_test)\n",
    "\n",
    "    x_train = x[list_train]\n",
    "    x_val = x[list_val]\n",
    "    x_test = x[list_test]\n",
    "\n",
    "    y_train = y[list_train]\n",
    "    y_val = y[list_val]\n",
    "    y_test = y[list_test]\n",
    "\n",
    "    print(\"x_train: \", x_train.shape)\n",
    "    print(\"y_train: \", y_train.shape)\n",
    "    print(\"x_val: \", x_val.shape)\n",
    "    print(\"y_val: \", y_val.shape)\n",
    "    print(\"x_test: \", x_test.shape)\n",
    "    print(\"y_test: \", y_test.shape)\n",
    "\n",
    "    input_shape = x_train[0].shape\n",
    "    model = my_custom_unet(\n",
    "            input_shape,\n",
    "            num_classes=2,\n",
    "            filters=64,\n",
    "            output_activation='softmax',\n",
    "            num_layers=4  \n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(f\"saved_objects/unet\"):\n",
    "\n",
    "\n",
    "        model_filename = f\"saved_objects/unet\"\n",
    "\n",
    "        callback_checkpoint = ModelCheckpoint(\n",
    "            model_filename, \n",
    "            verbose=1, \n",
    "            monitor='val_loss', \n",
    "            save_best_only=True\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=0.0001), \n",
    "            loss = weighted_categorical_crossentropy(weights),\n",
    "            metrics=[iou]\n",
    "        )\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        train_gen = get_augmented(x_train, y_train, batch_size=2,data_gen_args=data_gen_args)\n",
    "        generator = my_generator(x_train, y_train,train_gen, batch_size=2,data_gen_args=data_gen_args)\n",
    "\n",
    "        history = model.fit_generator(\n",
    "            generator,\n",
    "            steps_per_epoch=30,\n",
    "            epochs=150,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=[callback_checkpoint]\n",
    "        )\n",
    "\n",
    "\n",
    "    model.load_weights(f\"saved_objects/unet\")\n",
    "    \n",
    "    GENERATED_UNET_DIR_PATH = f\"{OUTPUT_DIR_PATH}/generated_unet\"\n",
    "    if os.path.exists(GENERATED_UNET_DIR_PATH):\n",
    "        shutil.rmtree(GENERATED_UNET_DIR_PATH)\n",
    "    os.makedirs(GENERATED_UNET_DIR_PATH)\n",
    "    \n",
    "    for N in range(x_test.shape[0]):\n",
    "        y_pred = model.predict(x_test[N:N+1])\n",
    "        predictions = np.round(y_pred)\n",
    "        for k in range(1,2):\n",
    "            dum = predictions[0,:,:,k]*255\n",
    "            filename = os.path.basename(orgs[list_test[N]])[:-4]\n",
    "            cv2.imwrite(f\"{GENERATED_UNET_DIR_PATH}/{filename}.png\",dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.future.graph as skigraph\n",
    "import shutil\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from kgcnn.utils.data import ragged_tensor_from_nested_numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import DBSCAN\n",
    "from PIL import Image, ImageOps\n",
    "import radiomics\n",
    "import numpy.ma as ma\n",
    "import nrrd\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script parameterss\n",
    "DATA_SET_DIR_NAMES = [\n",
    "    #\"250_pixel_15_com\",\n",
    "    #\"250_pixel_25_com\",\n",
    "    #\"250_pixel_35_com\",\n",
    "    \"300_pixel_10_com\",  \n",
    "    #\"300_pixel_20_com\",  \n",
    "    #\"300_pixel_30_com\",  \n",
    "    #\"300_pixel_40_com\",\n",
    "    #\"250_pixel_20_com\",\n",
    "    #\"250_pixel_30_com\", \n",
    "    #\"250_pixel_40_com\",  \n",
    "    #\"300_pixel_15_com\",  \n",
    "    #\"300_pixel_25_com\", \n",
    "    #\"300_pixel_35_com\",  \n",
    "    #\"300_pixel_5_com\"\n",
    "]\n",
    "PYRADIOMICS_FEATURES = [\n",
    "    \"original_firstorder_Mean\",\n",
    "    \"original_firstorder_10Percentile\",\n",
    "    \"original_firstorder_90Percentile\",\n",
    "    \"original_firstorder_Entropy\",\n",
    "    \"original_firstorder_InterquartileRange\",\n",
    "    \"original_firstorder_Kurtosis\",\n",
    "    \"original_firstorder_Maximum\",\n",
    "    \"original_firstorder_MeanAbsoluteDeviation\",\n",
    "    \"original_firstorder_Median\",\n",
    "    \"original_firstorder_Minimum\",\n",
    "    \"original_firstorder_Range\",\n",
    "    \"original_firstorder_RobustMeanAbsoluteDeviation\",\n",
    "    \"original_firstorder_RootMeanSquared\",\n",
    "    \"original_firstorder_Skewness\",\n",
    "    \"original_firstorder_Uniformity\",\n",
    "    \"original_firstorder_Variance\",\n",
    "    \"original_glcm_Autocorrelation\",\n",
    "    \"original_glcm_ClusterProminence\",\n",
    "    \"original_glcm_ClusterShade\",\n",
    "    \"original_glcm_ClusterTendency\",\n",
    "    \"original_glcm_Contrast\",\n",
    "    \"original_glcm_Correlation\",\n",
    "    \"original_glcm_DifferenceAverage\",\n",
    "    \"original_glcm_DifferenceEntropy\",\n",
    "    \"original_glcm_DifferenceVariance\",\n",
    "    \"original_glcm_Id\",\n",
    "    \"original_glcm_Idm\",\n",
    "    \"original_glcm_Idmn\",\n",
    "    \"original_glcm_Idn\",\n",
    "    \"original_glcm_Imc1\",\n",
    "    \"original_glcm_Imc2\",\n",
    "    \"original_glcm_InverseVariance\",\n",
    "    \"original_glcm_JointAverage\",\n",
    "    \"original_glcm_JointEnergy\",\n",
    "    \"original_glcm_JointEntropy\",\n",
    "    \"original_glcm_MCC\",\n",
    "    \"original_glcm_MaximumProbability\",\n",
    "    \"original_glcm_SumAverage\",\n",
    "    \"original_glcm_SumEntropy\",\n",
    "    \"original_glcm_SumSquares\",\n",
    "    \"original_gldm_DependenceEntropy\",\n",
    "    \"original_gldm_DependenceNonUniformity\",\n",
    "    \"original_gldm_DependenceNonUniformityNormalized\",\n",
    "    \"original_gldm_DependenceVariance\",\n",
    "    \"original_gldm_GrayLevelNonUniformity\",\n",
    "    \"original_gldm_GrayLevelVariance\",\n",
    "    \"original_gldm_HighGrayLevelEmphasis\",\n",
    "    \"original_gldm_LargeDependenceEmphasis\",\n",
    "    \"original_gldm_LargeDependenceLowGrayLevelEmphasis\",\n",
    "    \"original_gldm_LowGrayLevelEmphasis\",\n",
    "    \"original_gldm_SmallDependenceHighGrayLevelEmphasis\",\n",
    "    \"original_gldm_SmallDependenceLowGrayLevelEmphasis\",\n",
    "    \"original_glrlm_GrayLevelNonUniformity\",\n",
    "    \"original_glrlm_GrayLevelNonUniformityNormalized\",\n",
    "    \"original_glrlm_GrayLevelVariance\",\n",
    "    \"original_glrlm_HighGrayLevelRunEmphasis\",\n",
    "    \"original_glrlm_LongRunEmphasis\",\n",
    "    \"original_glrlm_LongRunHighGrayLevelEmphasis\",\n",
    "    \"original_glrlm_LongRunLowGrayLevelEmphasis\",\n",
    "    \"original_glrlm_LowGrayLevelRunEmphasis\",\n",
    "    \"original_glrlm_RunLengthNonUniformity\",\n",
    "    \"original_glrlm_RunLengthNonUniformityNormalized\",\n",
    "    \"original_glrlm_RunPercentage\",\n",
    "    \"original_glrlm_RunVariance\",\n",
    "    \"original_glrlm_ShortRunEmphasis\",\n",
    "    \"original_glrlm_ShortRunHighGrayLevelEmphasis\",\n",
    "    \"original_glrlm_ShortRunLowGrayLevelEmphasis\",\n",
    "    \"original_glszm_GrayLevelNonUniformity\",\n",
    "    \"original_glszm_GrayLevelNonUniformityNormalized\",\n",
    "    \"original_glszm_GrayLevelVariance\",\n",
    "    \"original_glszm_HighGrayLevelZoneEmphasis\",\n",
    "    \"original_glszm_LargeAreaEmphasis\",\n",
    "    \"original_glszm_LargeAreaHighGrayLevelEmphasis\",\n",
    "    \"original_glszm_LargeAreaLowGrayLevelEmphasis\",\n",
    "    \"original_glszm_LowGrayLevelZoneEmphasis\",\n",
    "    \"original_glszm_SizeZoneNonUniformity\",\n",
    "    \"original_glszm_SizeZoneNonUniformityNormalized\",\n",
    "    \"original_glszm_SmallAreaEmphasis\",\n",
    "    \"original_glszm_SmallAreaLowGrayLevelEmphasis\",\n",
    "    \"original_glszm_ZoneEntropy\",\n",
    "    \"original_glszm_ZonePercentage\",\n",
    "    \"original_glszm_ZoneVariance\",\n",
    "    \"original_ngtdm_Busyness\",\n",
    "    \"original_ngtdm_Coarseness\",\n",
    "    \"original_ngtdm_Complexity\",\n",
    "    \"original_ngtdm_Contrast\",\n",
    "    \"original_ngtdm_Strength\",\n",
    "]\n",
    "\n",
    "WHITE_SUPERPIXEL_LABEL_TRESHOLD = 0.65\n",
    "\n",
    "#configuration\n",
    "logger = logging.getLogger(\"radiomics\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def save_object(filename, obj):\n",
    "    obj_file = open(filename, \"wb\")\n",
    "    pickle.dump(obj, obj_file)\n",
    "    obj_file.close()\n",
    "    \n",
    "def load_object(filename):\n",
    "    obj_file = open(filename, \"rb\")\n",
    "    obj = pickle.load(obj_file)\n",
    "    obj_file.close()\n",
    "    \n",
    "    return obj\n",
    "\n",
    "# semantic segmentation using subgraphs of image graphs\n",
    "class Graph:\n",
    "    def __init__(self, file_id, rag, image, superpixels_labels, mask):\n",
    "        self.file_id = file_id\n",
    "        self.rag = rag\n",
    "        self.image = image\n",
    "        self.superpixels_labels = superpixels_labels\n",
    "        self.mask = mask\n",
    "\n",
    "class Subgraph:\n",
    "    def __init__(self, rag, graph, middle_superpixel_label, label):\n",
    "        self.rag = rag\n",
    "        self.graph = graph\n",
    "        self.middle_superpixel_label = middle_superpixel_label\n",
    "        self.label = label\n",
    "        self.unnormalized_edge_indices = None\n",
    "        self.normalized_edge_indices = None\n",
    "        self.edges = None\n",
    "        self.nodes = None\n",
    "        \n",
    "def assign_label(graph):\n",
    "    white_pixels_count = defaultdict(int)\n",
    "    total_pixels_count = defaultdict(int)\n",
    "    \n",
    "    for (i, row) in enumerate(graph.superpixels_labels):\n",
    "        for (j, superpixel_label) in enumerate(row):\n",
    "            total_pixels_count[superpixel_label] += 1\n",
    "            if graph.mask[i][j] == 1:\n",
    "                white_pixels_count[superpixel_label] += 1\n",
    "      \n",
    "    for node in graph.rag:\n",
    "        graph.rag.nodes[node]['label'] = 1.0 if white_pixels_count[node] / total_pixels_count[node] >= WHITE_SUPERPIXEL_LABEL_TRESHOLD else 0.0\n",
    "            \n",
    "            \n",
    "def assign_features(graph):\n",
    "    unique_superpixels_labels = np.unique(graph.superpixels_labels)\n",
    "    \n",
    "    for superpixel_label in unique_superpixels_labels:\n",
    "        superpixel_label_mask = (graph.superpixels_labels == superpixel_label).astype(int)\n",
    "        NRRD_DIRECTORY_PATH = f\"{OUTPUT_DIR_PATH}/nrrd\"\n",
    "        NRRD_ITEM_DIRECTORY_PATH = f\"{NRRD_DIRECTORY_PATH}/{graph.file_id}\"\n",
    "        os.makedirs(NRRD_ITEM_DIRECTORY_PATH, exist_ok=True)\n",
    "        \n",
    "        os.makedirs(f\"{NRRD_ITEM_DIRECTORY_PATH}\", exist_ok=True)\n",
    "        nrrd.write(f\"{NRRD_ITEM_DIRECTORY_PATH}/{superpixel_label}_image.nrrd\", graph.image)\n",
    "        nrrd.write(f\"{NRRD_ITEM_DIRECTORY_PATH}/{superpixel_label}_superpixel_label_mask.nrrd\", superpixel_label_mask)\n",
    "        nrrd_image_path = os.path.join(f\"{NRRD_ITEM_DIRECTORY_PATH}\", str(superpixel_label) + \"_image.nrrd\")\n",
    "        nrrd_superpixel_label_mask_path = os.path.join(f\"{NRRD_ITEM_DIRECTORY_PATH}\", str(superpixel_label) + \"_superpixel_label_mask.nrrd\")\n",
    "        \n",
    "        extractor = radiomics.featureextractor.RadiomicsFeatureExtractor(minimumROIDimensions=1)\n",
    "        result = extractor.execute(nrrd_image_path, nrrd_superpixel_label_mask_path) \n",
    "\n",
    "        for feature in PYRADIOMICS_FEATURES:\n",
    "            graph.rag.nodes[superpixel_label][feature] = result[feature]\n",
    "    \n",
    "def process_images():\n",
    "    EXPECTED_DIR_PATH = f\"{OUTPUT_DIR_PATH}/expected\"\n",
    "    if os.path.exists(EXPECTED_DIR_PATH):\n",
    "        shutil.rmtree(EXPECTED_DIR_PATH)\n",
    "    os.makedirs(EXPECTED_DIR_PATH)\n",
    "    \n",
    "    filenames = os.listdir(IMAGES_DIR_PATH)\n",
    "    graphs = list()\n",
    "        \n",
    "    for (file_count, filename) in enumerate(filenames, start=1):\n",
    "        print(f\"Processing files: {file_count}/{len(filenames)}\")\n",
    "        \n",
    "        file_id = os.path.splitext(filename)[0]\n",
    "        \n",
    "        try:\n",
    "            image = np.array(ImageOps.grayscale(Image.open(f\"{IMAGES_DIR_PATH}/{filename}\")))\n",
    "            mask = np.array(ImageOps.grayscale(Image.open(f\"{MASKS_DIR_PATH}/{file_id}.bmp\")))\n",
    "            superpixels_labels = np.fromfile(f\"{SUPERPIXELS_LABELS_DIR_PATH}/{file_id}.dat\", dtype=np.dtype((np.int32, image.shape)))[0]\n",
    "        except FileNotFoundError as error: \n",
    "            print(error)\n",
    "            \n",
    "        rag = skigraph.rag_mean_color(image, superpixels_labels)\n",
    "        graphs.append(Graph(file_id, rag, image, superpixels_labels, mask))\n",
    "        assign_label(graphs[-1])\n",
    "        assign_features(graphs[-1])\n",
    "        \n",
    "        expected = Image.fromarray((mask*255).astype(np.uint8))\n",
    "        expected.save(f\"{EXPECTED_DIR_PATH}/{file_id}.png\")\n",
    "        \n",
    "    print(\"All files have been processed\")\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "def split_into_subgraphs(graphs):\n",
    "    subgraphs = []\n",
    "    for graph in graphs:\n",
    "        for node in graph.rag.nodes:\n",
    "            nodes = [neighbor for neighbor in graph.rag.neighbors(node)] + [node]\n",
    "            rag = graph.rag.subgraph(nodes)\n",
    "            label = graph.rag.nodes[node]['label']\n",
    "            subgraphs.append(Subgraph(rag, graph, node, label))\n",
    "            \n",
    "    return subgraphs\n",
    "\n",
    "def normalize_edge_indices(edge_indices):\n",
    "    flat_list = [node for edge_index in edge_indices for node in edge_index]            \n",
    "    flat_list.sort()\n",
    "    flat_list = list(dict.fromkeys(flat_list))\n",
    "    change = {key:value for (value, key) in enumerate(flat_list)}\n",
    "    result = [[change[edge_index[0]], change[edge_index[1]]] for edge_index in edge_indices] \n",
    "    \n",
    "    return result\n",
    "\n",
    "def prepare_data(subgraphs):\n",
    "    nodes = []\n",
    "    edge_indices = []\n",
    "    edges = []\n",
    "    labels = []\n",
    "    \n",
    "    for subgraph in subgraphs:\n",
    "        node_features = []\n",
    "        \n",
    "        for node in subgraph.rag.nodes:\n",
    "            node_features.append([subgraph.rag.nodes[node][feature] for feature in PYRADIOMICS_FEATURES])\n",
    "            \n",
    "        nodes.append(node_features)\n",
    "        edges.append([[1.0] for edge in subgraph.rag.edges.data()])\n",
    "        unnormalized_edge_indices = [list(index) for index in subgraph.rag.edges]\n",
    "        edge_indices.append(normalize_edge_indices(unnormalized_edge_indices))\n",
    "        labels.append(subgraph.label)\n",
    "        \n",
    "        subgraph.nodes = nodes[-1]\n",
    "        subgraph.unnormalized_edge_indices = unnormalized_edge_indices\n",
    "        subgraph.normalized_edge_indices = edge_indices[-1]\n",
    "        subgraph.edges = edges[-1]\n",
    "\n",
    "    return nodes, edge_indices, edges, np.array(labels)\n",
    "\n",
    "def generate_segmented_images_from_predictions(subgraphs, predictions):\n",
    "    predicted_images = {}\n",
    "    GENERATED_DIR_PATH = f\"{OUTPUT_DIR_PATH}/generated\"\n",
    "    if os.path.exists(GENERATED_DIR_PATH):\n",
    "        shutil.rmtree(GENERATED_DIR_PATH)\n",
    "    os.makedirs(GENERATED_DIR_PATH)\n",
    "\n",
    "    for (prediction_count, subgraph) in enumerate(subgraphs):\n",
    "        if subgraph.graph.file_id not in predicted_images:\n",
    "            predicted_images[subgraph.graph.file_id] = np.copy(subgraph.graph.superpixels_labels)\n",
    "        predicted_images[subgraph.graph.file_id][predicted_images[subgraph.graph.file_id] == subgraph.middle_superpixel_label] = predictions[prediction_count] - 2\n",
    "\n",
    "    for file_id in predicted_images.keys():\n",
    "        predicted_images[file_id][predicted_images[file_id] == -1] = 255\n",
    "        predicted_images[file_id][predicted_images[file_id] == -2] = 0\n",
    "        generated = Image.fromarray(predicted_images[file_id].astype(np.uint8))\n",
    "        generated.save(f\"{GENERATED_DIR_PATH}/{file_id}.png\")\n",
    "\n",
    "def compute_dice_coefficient(expected, generated):\n",
    "    expected[expected!=0] = 1\n",
    "    generated[generated!=0] = 1\n",
    "    \n",
    "    return 2*np.sum(expected*generated)/(np.sum(expected)+np.sum(generated))\n",
    "\n",
    "def compute_dice_coefficients_for_test_set(graphs_test, info_file):\n",
    "    info_file.write(f\"\\t\\tGCN\\t\\t\\tUNET\\n\")\n",
    "    for graph in graphs_test:\n",
    "        expected = np.array(ImageOps.grayscale(Image.open(f\"{OUTPUT_DIR_PATH}/expected/{graph.file_id}.png\")))\n",
    "        generated = np.array(ImageOps.grayscale(Image.open(f\"{OUTPUT_DIR_PATH}/generated/{graph.file_id}.png\")))\n",
    "        generated_unet = np.array(ImageOps.grayscale(Image.open(f\"{OUTPUT_DIR_PATH}/generated_unet/{graph.file_id}.png\")))\n",
    "        \n",
    "        dice_coefficient = compute_dice_coefficient(expected, generated)\n",
    "        dice_coefficient_unet = compute_dice_coefficient(expected, generated_unet)\n",
    "        \n",
    "        info_file.write(f\"{graph.file_id}\\t{str(dice_coefficient)}\\t{str(dice_coefficient_unet)}\\n\")\n",
    "\n",
    "def build_model(hp):\n",
    "    model = make_model(\n",
    "            name= \"GraphSAGE\",\n",
    "            inputs= [{'shape': (None,len(PYRADIOMICS_FEATURES)), 'name': \"node_attributes\", 'dtype': 'float32', 'ragged': True},\n",
    "                    {'shape': (None,), 'name': \"edge_attributes\", 'dtype': 'float32', 'ragged': True},\n",
    "                    {'shape': (None, 2), 'name': \"edge_indices\", 'dtype': 'int64', 'ragged': True}],\n",
    "            input_embedding = {\n",
    "                                \"node\": \n",
    "                                    {\n",
    "                                        \"input_dim\": 15,\n",
    "                                        \"output_dim\": 15\n",
    "                                    },\n",
    "                                \"edge\": {\"input_dim\": 1, \"output_dim\": 1}\n",
    "            },\n",
    "            output_embedding= 'graph',\n",
    "            output_mlp = {\n",
    "                            \"use_bias\": [\n",
    "                                        True, \n",
    "                                        True, \n",
    "                                        False\n",
    "                            ],\n",
    "                            \"units\": [\n",
    "                                        25,\n",
    "                                        10,\n",
    "                                        1\n",
    "                            ],\n",
    "                             \"activation\": [\n",
    "                                                \"relu\",\n",
    "                                                \"relu\",\n",
    "                                                \"sigmoid\"\n",
    "                                            ]\n",
    "            },\n",
    "            node_mlp_args = {\n",
    "                                \"units\": [\n",
    "                                    25,\n",
    "                                    20,\n",
    "                                ], \n",
    "                                \"use_bias\": [\n",
    "                                        True, \n",
    "                                        True, \n",
    "                                ], \n",
    "                                \"activation\": [\n",
    "                                    \"linear\",\n",
    "                                    \"linear\",\n",
    "                                ]\n",
    "            },\n",
    "            pooling_args = {\n",
    "                            'pooling_method': \"segment_mean\",\n",
    "            },\n",
    "            pooling_nodes_args = {'pooling_method': 'mean'},\n",
    "            gather_args = {},\n",
    "            concat_args = {\n",
    "                            \"axis\": -1\n",
    "            },\n",
    "            use_edge_features = False,\n",
    "            depth = 1,\n",
    "            verbose = 1\n",
    "        )\n",
    "    learning_rate_start = 5e-3\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate_start)\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=optimizer,\n",
    "                  weighted_metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "def train_GCN(graphs, info_file, graphs_train, graphs_val, graphs_test, use_saved_model):\n",
    "    subgraphs_train = split_into_subgraphs(graphs_train)\n",
    "    subgraphs_val = split_into_subgraphs(graphs_val)\n",
    "    subgraphs_test = split_into_subgraphs(graphs_test)\n",
    "\n",
    "    nodes_train, edge_indices_train, edges_train, labels_train =  prepare_data(subgraphs_train)\n",
    "    nodes_val, edge_indices_val, edges_val, labels_val =  prepare_data(subgraphs_val)\n",
    "    nodes_test, edge_indices_test, edges_test, labels_test =  prepare_data(subgraphs_test)\n",
    "\n",
    "    nodes_train = ragged_tensor_from_nested_numpy(nodes_train)\n",
    "    edges_train = ragged_tensor_from_nested_numpy(edges_train)\n",
    "    edge_indices_train = ragged_tensor_from_nested_numpy(edge_indices_train)\n",
    "    \n",
    "    nodes_val = ragged_tensor_from_nested_numpy(nodes_val)\n",
    "    edges_val = ragged_tensor_from_nested_numpy(edges_val)\n",
    "    edge_indices_val = ragged_tensor_from_nested_numpy(edge_indices_val)\n",
    "\n",
    "    nodes_test = ragged_tensor_from_nested_numpy(nodes_test)\n",
    "    edges_test = ragged_tensor_from_nested_numpy(edges_test)\n",
    "    edge_indices_test = ragged_tensor_from_nested_numpy(edge_indices_test)\n",
    "\n",
    "    xtrain = nodes_train, edges_train, edge_indices_train\n",
    "    xval = nodes_val, edges_val, edge_indices_val\n",
    "    xtest = nodes_test, edges_test, edge_indices_test\n",
    "    ytrain = labels_train\n",
    "    yval = labels_val\n",
    "    ytest = labels_test\n",
    "    print([x.shape for x in xtrain])\n",
    "    print([x.shape for x in xtest])\n",
    "    print([x.shape for x in xval])\n",
    "    print(ytrain.shape, ytest.shape)\n",
    "    info_file.write(f\"nodes_train: {xval[0].shape}\\n\")\n",
    "    info_file.write(f\"edges_train: {xval[1].shape}\\n\")\n",
    "    info_file.write(f\"edge_indices_train: {xval[2].shape}\\n\")\n",
    "    info_file.write(f\"labels_train: {yval.shape}\\n\")\n",
    "    \n",
    "    info_file.write(f\"nodes_val: {xtrain[0].shape}\\n\")\n",
    "    info_file.write(f\"edges_val: {xtrain[1].shape}\\n\")\n",
    "    info_file.write(f\"edge_indices_val: {xtrain[2].shape}\\n\")\n",
    "    info_file.write(f\"labels_val: {ytrain.shape}\\n\")\n",
    "    \n",
    "    info_file.write(f\"nodes_test: {xtest[0].shape}\\n\")\n",
    "    info_file.write(f\"edges_test: {xtest[1].shape}\\n\")\n",
    "    info_file.write(f\"edge_indices_test: {xtest[2].shape}\\n\")\n",
    "    info_file.write(f\"labels_test: {ytest.shape}\\n\")\n",
    "    \n",
    "    MODEL_FILE_PATH = f\"{SAVED_OBJECTS_DIR_PATH}/model\"\n",
    "    if os.path.exists(MODEL_FILE_PATH) and use_saved_model:\n",
    "        model = keras.models.load_model(MODEL_FILE_PATH)\n",
    "    else:        \n",
    "        learning_rate_start = 1e-6\n",
    "        learning_rate_stop = 1e-7\n",
    "        epo = 150\n",
    "        epostep = 1\n",
    "        \n",
    "        model_filename = f\"saved_objects/gs\"\n",
    "\n",
    "        callback_checkpoint = ModelCheckpoint(\n",
    "            model_filename, \n",
    "            verbose=1, \n",
    "            monitor='val_loss', \n",
    "            save_best_only=True\n",
    "        )\n",
    "        '''\n",
    "        tuner = kt.RandomSearch(\n",
    "            build_model,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=5000\n",
    "        )\n",
    "        \n",
    "        class_weight = {\n",
    "            0: len([subgraph for subgraph in subgraphs_train if subgraph.label == 0.]) / len(subgraphs_train),\n",
    "            1: len([subgraph for subgraph in subgraphs_train if subgraph.label == 1.]) / len(subgraphs_train)\n",
    "        }\n",
    "        '''\n",
    "        '''\n",
    "        res = tuner.search(xtrain, ytrain,\n",
    "            epochs = epo,\n",
    "            batch_size = 32,\n",
    "            validation_data = (xval, yval),\n",
    "        )\n",
    "        \n",
    "        best_hps = tuner.get_best_hyperparameters(5)\n",
    "        print(best_hps)\n",
    "        print(best_hps[0])\n",
    "        print(tuner.results_summary())\n",
    "        epo = 50\n",
    "        '''\n",
    "        model = build_model(None)\n",
    "\n",
    "        \n",
    "        start = time.process_time()\n",
    "        hist = model.fit(xtrain, ytrain,\n",
    "            epochs=epo,\n",
    "            batch_size=32,\n",
    "            validation_data=(xval, yval),\n",
    "            callbacks = [callback_checkpoint]\n",
    "        )\n",
    "        \n",
    "        stop = time.process_time()\n",
    "        print(\"Print Time for taining: \", stop - start)\n",
    "        \n",
    "        trainlossall = np.array(hist.history['accuracy'])\n",
    "        testlossall = np.array(hist.history['val_accuracy'])\n",
    "        acc_valid = testlossall[-1]\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(trainlossall.shape[0]), trainlossall, label='Training ACC', c='blue')\n",
    "        plt.plot(np.arange(epostep, epo + epostep, epostep), testlossall, label='Test ACC', c='red')\n",
    "        plt.scatter([trainlossall.shape[0]], [acc_valid], label=\"{0:0.4f} \".format(acc_valid), c='red')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Interaction Network Loss')\n",
    "        plt.legend(loc='upper right', fontsize='x-large')\n",
    "        \n",
    "        \n",
    "        TRAINING_PLOT_FILE_PATH = f\"{OUTPUT_DIR_PATH}/training_plot.png\"\n",
    "        if os.path.exists(TRAINING_PLOT_FILE_PATH):\n",
    "            os.remove(TRAINING_PLOT_FILE_PATH)\n",
    "        plt.savefig(TRAINING_PLOT_FILE_PATH)\n",
    "        plt.show()\n",
    "        \n",
    "        model.load_weights(model_filename)\n",
    "        model.save(MODEL_FILE_PATH)\n",
    "    \n",
    "    probability = model.predict(xtest)\n",
    "    predictions = np.round(probability)\n",
    "    print(confusion_matrix(ytest, predictions))\n",
    "    info_file.write(np.array2string(confusion_matrix(ytest, predictions))+\"\\n\")\n",
    "    generate_segmented_images_from_predictions(subgraphs_test, predictions)\n",
    "\n",
    "def initialize_dir_paths(data_set_dir_name):\n",
    "    # globals\n",
    "    global DATA_DIR_PATH \n",
    "    global IMAGES_DIR_PATH\n",
    "    global SUPERPIXELS_LABELS_DIR_PATH\n",
    "    global MASKS_DIR_PATH\n",
    "    global OUTPUT_DIR_PATH\n",
    "    global SAVED_OBJECTS_DIR_PATH\n",
    "    \n",
    "    DATA_DIR_PATH = f\"./input/{data_set_dir_name}\"\n",
    "    IMAGES_DIR_PATH = f\"{DATA_DIR_PATH}/images\"\n",
    "    SUPERPIXELS_LABELS_DIR_PATH = f\"{DATA_DIR_PATH}/superpixels_labels\"\n",
    "    MASKS_DIR_PATH = f\"{DATA_DIR_PATH}/masks\"\n",
    "    \n",
    "    OUTPUT_DIR_PATH = f\"./output/{data_set_dir_name}\"\n",
    "    SAVED_OBJECTS_DIR_PATH = f\"./saved_objects/{data_set_dir_name}\"\n",
    "    \n",
    "def process_data_set(data_set_dir_name, use_saved_graphs=True, use_saved_model=True):  \n",
    "    initialize_dir_paths(data_set_dir_name)\n",
    "    os.makedirs(f\"{OUTPUT_DIR_PATH}\", exist_ok=True)\n",
    "    os.makedirs(f\"{SAVED_OBJECTS_DIR_PATH}\", exist_ok=True)\n",
    "    \n",
    "    GRAPHS_FILE_PATH = f\"{SAVED_OBJECTS_DIR_PATH}/graphs\"\n",
    "    if os.path.exists(GRAPHS_FILE_PATH) and use_saved_graphs:\n",
    "        graphs = load_object(GRAPHS_FILE_PATH)\n",
    "    else:\n",
    "        graphs = process_images()\n",
    "        save_object(GRAPHS_FILE_PATH, graphs)\n",
    "\n",
    "    graphs_training, graphs_test = train_test_split(graphs, train_size=0.8, random_state=1)\n",
    "    graphs_train, graphs_val = train_test_split(graphs_training, train_size=0.9, random_state=42)\n",
    "\n",
    "    INFO_FILE_PATH = f\"{OUTPUT_DIR_PATH}/info.txt\"\n",
    "    if os.path.exists(INFO_FILE_PATH):\n",
    "        os.remove(INFO_FILE_PATH)\n",
    "    with open(INFO_FILE_PATH,\"w+\") as info_file:\n",
    "        info_file.write(f\"data set directory name: {data_set_dir_name}\\n\")\n",
    "        \n",
    "        train_GCN(graphs, info_file, graphs_train, graphs_val, graphs_test, use_saved_model)\n",
    "        train_UNET(graphs_train, graphs_test)\n",
    "        compute_dice_coefficients_for_test_set(graphs_test, info_file)\n",
    "    \n",
    "def process_data_sets(use_saved_graphs=True, use_saved_model=True):\n",
    "    for data_set_dir_name in DATA_SET_DIR_NAMES:\n",
    "        process_data_set(data_set_dir_name, use_saved_graphs, use_saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([24969, None, 87]), TensorShape([24969, None, 1]), TensorShape([24969, None, 2])]\n",
      "[TensorShape([7583, None, 87]), TensorShape([7583, None, 1]), TensorShape([7583, None, 2])]\n",
      "[TensorShape([3086, None, 87]), TensorShape([3086, None, 1]), TensorShape([3086, None, 2])]\n",
      "(24969,) (7583,)\n",
      "INFO:kgcnn: Updated model kwargs:\n",
      "{'concat_args': {'axis': -1},\n",
      " 'depth': 1,\n",
      " 'edge_mlp_args': {'activation': ['relu', 'linear'],\n",
      "                   'units': [100, 50],\n",
      "                   'use_bias': True},\n",
      " 'gather_args': {},\n",
      " 'input_embedding': {'edge': {'input_dim': 1, 'output_dim': 1},\n",
      "                     'node': {'input_dim': 15, 'output_dim': 15}},\n",
      " 'inputs': [{'dtype': 'float32',\n",
      "             'name': 'node_attributes',\n",
      "             'ragged': True,\n",
      "             'shape': (None, 87)},\n",
      "            {'dtype': 'float32',\n",
      "             'name': 'edge_attributes',\n",
      "             'ragged': True,\n",
      "             'shape': (None,)},\n",
      "            {'dtype': 'int64',\n",
      "             'name': 'edge_indices',\n",
      "             'ragged': True,\n",
      "             'shape': (None, 2)}],\n",
      " 'name': 'GraphSAGE',\n",
      " 'node_mlp_args': {'activation': ['linear', 'linear'],\n",
      "                   'units': [25, 20],\n",
      "                   'use_bias': [True, True]},\n",
      " 'output_embedding': 'graph',\n",
      " 'output_mlp': {'activation': ['relu', 'relu', 'sigmoid'],\n",
      "                'units': [25, 10, 1],\n",
      "                'use_bias': [True, True, False]},\n",
      " 'pooling_args': {'pooling_method': 'segment_mean'},\n",
      " 'pooling_nodes_args': {'pooling_method': 'mean'},\n",
      " 'use_edge_features': False,\n",
      " 'verbose': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:kgcnn: Updated model kwargs:\n",
      "{'concat_args': {'axis': -1},\n",
      " 'depth': 1,\n",
      " 'edge_mlp_args': {'activation': ['relu', 'linear'],\n",
      "                   'units': [100, 50],\n",
      "                   'use_bias': True},\n",
      " 'gather_args': {},\n",
      " 'input_embedding': {'edge': {'input_dim': 1, 'output_dim': 1},\n",
      "                     'node': {'input_dim': 15, 'output_dim': 15}},\n",
      " 'inputs': [{'dtype': 'float32',\n",
      "             'name': 'node_attributes',\n",
      "             'ragged': True,\n",
      "             'shape': (None, 87)},\n",
      "            {'dtype': 'float32',\n",
      "             'name': 'edge_attributes',\n",
      "             'ragged': True,\n",
      "             'shape': (None,)},\n",
      "            {'dtype': 'int64',\n",
      "             'name': 'edge_indices',\n",
      "             'ragged': True,\n",
      "             'shape': (None, 2)}],\n",
      " 'name': 'GraphSAGE',\n",
      " 'node_mlp_args': {'activation': ['linear', 'linear'],\n",
      "                   'units': [25, 20],\n",
      "                   'use_bias': [True, True]},\n",
      " 'output_embedding': 'graph',\n",
      " 'output_mlp': {'activation': ['relu', 'relu', 'sigmoid'],\n",
      "                'units': [25, 10, 1],\n",
      "                'use_bias': [True, True, False]},\n",
      " 'pooling_args': {'pooling_method': 'segment_mean'},\n",
      " 'pooling_nodes_args': {'pooling_method': 'mean'},\n",
      " 'use_edge_features': False,\n",
      " 'verbose': 1}\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_1/pooling_local_edges_1/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_1/pooling_local_edges_1/Reshape:0\", shape=(None, 50), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_1/pooling_local_edges_1/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/781 [====>.........................] - ETA: 4s - loss: 0.3847 - accuracy: 0.8230"
     ]
    }
   ],
   "source": [
    "process_data_sets(use_saved_graphs=True, use_saved_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
